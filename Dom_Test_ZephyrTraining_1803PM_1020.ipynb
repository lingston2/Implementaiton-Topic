{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eae4861-4283-43f9-a0a9-9b16ad8e46e9",
      "metadata": {
        "id": "9eae4861-4283-43f9-a0a9-9b16ad8e46e9",
        "outputId": "10d04d80-6a5d-4b5d-fe16-133e09986864"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 2.19 s, sys: 2.4 s, total: 4.59 s\n",
            "Wall time: 1.92 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        "    LlamaForCausalLM\n",
        ")\n",
        "from peft import LoraConfig, PeftModel, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "import sentencepiece\n",
        "import huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dccf841-b91d-4ccf-8fe2-588c1e5c9b22",
      "metadata": {
        "id": "5dccf841-b91d-4ccf-8fe2-588c1e5c9b22",
        "outputId": "1a1ead72-92e0-4059-814e-c942c46803d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 138 ms, sys: 9.02 ms, total: 147 ms\n",
            "Wall time: 6.56 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# The model that you want to train from the Hugging Face hub\n",
        "#model_name = \"georgepullen/Llama-2-13b-hf-sharded-bf16-1GB\"\n",
        "# model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "# model_name = \"NousResearch/Llama-2-7b-hf\"\n",
        "model_name = \"HuggingFaceH4/zephyr-7b-alpha\"\n",
        "\n",
        "# The instruction dataset to use\n",
        "dataset_name = \"bobbybelajar/AmazonMixedLength\"\n",
        "# Fine-tuned model name\n",
        "# new_model = \"llama2-7b-amazon-mix-epoch4-2e-4\"\n",
        "new_model = \"zephyr-7b-amazon-mix-epoch4-2e-4-dom\"\n",
        "################################################################################\n",
        "# QLoRA parameters\n",
        "################################################################################\n",
        "\n",
        "# LoRA attention dimension\n",
        "lora_r = 64\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 16\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.1\n",
        "\n",
        "################################################################################\n",
        "# bitsandbytes parameters\n",
        "################################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "################################################################################\n",
        "# TrainingArguments parameters\n",
        "################################################################################\n",
        "\n",
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 4\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 1\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 1\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning rate schedule (constant a bit better than cosine)\n",
        "lr_scheduler_type = \"constant\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = 2500\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 25\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 25\n",
        "\n",
        "################################################################################\n",
        "# SFT parameters\n",
        "################################################################################\n",
        "\n",
        "# Maximum sequence length to use\n",
        "max_seq_length = 2048\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}\n",
        "\n",
        "# Load dataset (you can process it here)\n",
        "dataset = load_dataset(dataset_name, split=\"train\")\n",
        "dataset2 = load_dataset(dataset_name, split=\"validation\")\n",
        "\n",
        "# Load tokenizer and model with QLoRA configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c54efbe-565a-4f35-8454-3e096811933e",
      "metadata": {
        "scrolled": true,
        "id": "0c54efbe-565a-4f35-8454-3e096811933e",
        "outputId": "1c1b9547-2986-4817-b0c3-b84968b60778",
        "colab": {
          "referenced_widgets": [
            "e826e57371684f64aa2b80534176beaf",
            "a6d44618d0bf4381b2cfdf3b9628fa95",
            "88a477de8c8947a0bcaddbef461fe043",
            "01acac397bd94300ad65b26a14779b0c",
            "688a8eaebd6149818644282f623c4978",
            "ece335f7dda9444eba72e117eda85ff7",
            "f218f74d83db41b88a50a7d660a45690",
            "91d6d9f1942b4febae8c783a367d6195",
            "65bad0880b5941fba5097de1702275cf",
            "449d127aa5524291bac20c21e19f5c28",
            "4636584e82454e75a15a6b58f8b1e637",
            "d87865bcca04422cbb4409bae155bd92",
            "182a754ae8154162a494d7dd12c59567",
            "5ee5020cfe204f4fb7774967975db1ba"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Your GPU supports bfloat16: accelerate training with bf16=True\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e826e57371684f64aa2b80534176beaf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/628 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6d44618d0bf4381b2cfdf3b9628fa95",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)fetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88a477de8c8947a0bcaddbef461fe043",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01acac397bd94300ad65b26a14779b0c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "688a8eaebd6149818644282f623c4978",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ece335f7dda9444eba72e117eda85ff7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f218f74d83db41b88a50a7d660a45690",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91d6d9f1942b4febae8c783a367d6195",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "65bad0880b5941fba5097de1702275cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "449d127aa5524291bac20c21e19f5c28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4636584e82454e75a15a6b58f8b1e637",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)of-00008.safetensors:   0%|          | 0.00/816M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d87865bcca04422cbb4409bae155bd92",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "182a754ae8154162a494d7dd12c59567",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/918 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ee5020cfe204f4fb7774967975db1ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/262 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using 8-bit optimizers with a version of `bitsandbytes` < 0.41.1. It is recommended to update your version as a major bug has been fixed in 8-bit optimizers.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "START TRAINING\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3672' max='3672' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3672/3672 24:53, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.340800</td>\n",
              "      <td>1.489257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.951100</td>\n",
              "      <td>1.599874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.503200</td>\n",
              "      <td>1.915236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.279800</td>\n",
              "      <td>2.095078</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FINISH TRAINING\n",
            "CPU times: user 20min 6s, sys: 6min 50s, total: 26min 57s\n",
            "Wall time: 29min 24s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Load LLaMA tokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "# tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
        "\n",
        "# Misrtral Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    padding_side=\"left\",\n",
        "    add_eos_token=True,\n",
        "    add_bos_token=True,\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load LoRA configuration\n",
        "# peft_config = LoraConfig(\n",
        "#     lora_alpha=lora_alpha,\n",
        "#     lora_dropout=lora_dropout,\n",
        "#     r=lora_r,\n",
        "#     bias=\"none\",\n",
        "#     task_type=\"CAUSAL_LM\",\n",
        "# )\n",
        "\n",
        "## Setting Lora for Mistral\n",
        "config = LoraConfig(\n",
        "    r=lora_r,\n",
        "    lora_alpha=lora_alpha,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"lm_head\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=lora_dropout,  # Conventional\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    # evaluation_strategy=\"steps\",\n",
        "    # eval_steps = 500,\n",
        "    # warmup_steps = 5,\n",
        "    # save_strategy=\"steps\",\n",
        "    # max_steps=max_steps,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=dataset2,\n",
        "    # peft_config=peft_config,\n",
        "    dataset_text_field=\"prompt_and_question\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        ")\n",
        "\n",
        "print(\"START TRAINING\")\n",
        "# Train model\n",
        "trainer.train()\n",
        "print(\"FINISH TRAINING\")\n",
        "# Save trained model\n",
        "trainer.model.save_pretrained(new_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4046cf22-79c2-4069-955a-5487c862398c",
      "metadata": {
        "id": "4046cf22-79c2-4069-955a-5487c862398c"
      },
      "outputs": [],
      "source": [
        "#After Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17e985b0-c5f0-4094-9d31-fa43a13d83dc",
      "metadata": {
        "id": "17e985b0-c5f0-4094-9d31-fa43a13d83dc",
        "outputId": "dde50368-447d-47fb-9c2a-36f0ff5ce187"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 12 µs, sys: 4 µs, total: 16 µs\n",
            "Wall time: 19.6 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        "    LlamaForCausalLM\n",
        ")\n",
        "from peft import LoraConfig, PeftModel, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "import sentencepiece\n",
        "import huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4276823c-42a4-4261-8b69-618be61c4d03",
      "metadata": {
        "scrolled": true,
        "id": "4276823c-42a4-4261-8b69-618be61c4d03",
        "outputId": "727b5275-9c48-4266-924a-907c8b04f710",
        "colab": {
          "referenced_widgets": [
            "20a119c38daf4b2eb4ad7f0a3073d642",
            "d834a8e72642451ba5cd7af381e4f6ed",
            "cb020fcecf6f4994ac549c7014d10340",
            "83efb040e3ef43f9b987eca837bf5e61",
            "301d18a49fde4c20bd45d195e70962a9",
            "ef071ba8108a46b0ae2d415eed69254a",
            "fe9025ab7bad48a4bf93e7c9e114e61d"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /home/dominick/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20a119c38daf4b2eb4ad7f0a3073d642",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d834a8e72642451ba5cd7af381e4f6ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00001-of-00004.bin:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb020fcecf6f4994ac549c7014d10340",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00002-of-00004.bin:   0%|          | 0.00/3.93G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83efb040e3ef43f9b987eca837bf5e61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00004-of-00004.bin:   0%|          | 0.00/2.68G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "301d18a49fde4c20bd45d195e70962a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef071ba8108a46b0ae2d415eed69254a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model-00003-of-00004.bin:   0%|          | 0.00/3.93G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe9025ab7bad48a4bf93e7c9e114e61d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/Domwerr/zephyr-7b-amazon-mix-epoch4-2e-4-dom/commit/b8994eaa89f9b871653c94a44688ed638c077c7d', commit_message='Upload tokenizer', commit_description='', oid='b8994eaa89f9b871653c94a44688ed638c077c7d', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "huggingface_hub.login(token = \"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\")\n",
        "\n",
        "# The model that you want to train from the Hugging Face hub\n",
        "model_name = \"HuggingFaceH4/zephyr-7b-alpha\"\n",
        "# model_name = \"NousResearch/Llama-2-7b-hf\"\n",
        "\n",
        "# The instruction dataset to use\n",
        "dataset_name = \"bobbybelajar/AmazonMixedLength\"\n",
        "# Fine-tuned model name\n",
        "# new_model = \"llama2-7b-amazon-mix-epoch4-2e-4\"\n",
        "new_model = \"zephyr-7b-amazon-mix-epoch4-2e-4-dom\"\n",
        "\n",
        "################################################################################\n",
        "# QLoRA parameters\n",
        "################################################################################\n",
        "\n",
        "# LoRA attention dimension\n",
        "lora_r = 64\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 16\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.1\n",
        "\n",
        "################################################################################\n",
        "# bitsandbytes parameters\n",
        "################################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "################################################################################\n",
        "# TrainingArguments parameters\n",
        "################################################################################\n",
        "\n",
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 4\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 2\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 2\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning rate schedule (constant a bit better than cosine)\n",
        "lr_scheduler_type = \"constant\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = 4\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 25\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 25\n",
        "\n",
        "################################################################################\n",
        "# SFT parameters\n",
        "################################################################################\n",
        "\n",
        "# Maximum sequence length to use\n",
        "max_seq_length = 2048\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, new_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# # Reload tokenizer to save it\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "# tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Misrtral Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    padding_side=\"left\",\n",
        "    add_eos_token=True,\n",
        "    add_bos_token=True,\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "new_model = \"zephyr-7b-amazon-mix-epoch4-2e-4-dom\"\n",
        "model.push_to_hub(new_model, max_shard_size = \"4000MB\",use_temp_dir=False)\n",
        "tokenizer.push_to_hub(new_model, max_shard_size = \"4000MB\", use_temp_dir=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "922fbd70-04e1-40c4-9361-2657f47204a1",
      "metadata": {
        "id": "922fbd70-04e1-40c4-9361-2657f47204a1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}